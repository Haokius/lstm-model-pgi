# -*- coding: utf-8 -*-
"""CIS 5450 Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u_y0OLFX7v5XWLfErZc15b1vrXLGw8dt

##CIS 5450 Project: LSTM for APPL Stock Price Prediction - Anand Majmudar, Kevin He, Terrance Ji

###Dataset###

AAPL.US_M30.csv

https://www.kaggle.com/datasets/olegshpagin/usa-stocks-prices-ohlcv/data

###Initialization###
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv("/content/AAPL.US_M30.csv")
df = df.tail(60000).reset_index(drop = True)

"""Display of raw data dataframe"""

raw_df = df.copy()
raw_df

"""Plotting the time series data (Note this is for 30 minute intervals)"""

raw_df['datetime'] = pd.to_datetime(raw_df['datetime'])

# Plotting each variable on separate plots
fig, axs = plt.subplots(5, 1, figsize=(10, 20))
fig.subplots_adjust(hspace=0.5)

axs[0].plot(raw_df['datetime'], raw_df['open'], label='Open', color='blue')
axs[0].set_title('Open Price Over Time')
axs[0].set_xlabel('Datetime')
axs[0].set_ylabel('Open Price')

axs[1].plot(raw_df['datetime'], raw_df['high'], label='High', color='green')
axs[1].set_title('High Price Over Time')
axs[1].set_xlabel('Datetime')
axs[1].set_ylabel('High Price')

axs[2].plot(raw_df['datetime'], raw_df['low'], label='Low', color='red')
axs[2].set_title('Low Price Over Time')
axs[2].set_xlabel('Datetime')
axs[2].set_ylabel('Low Price')

axs[3].plot(raw_df['datetime'], raw_df['close'], label='Close', color='purple')
axs[3].set_title('Close Price Over Time')
axs[3].set_xlabel('Datetime')
axs[3].set_ylabel('Close Price')

axs[4].plot(raw_df['datetime'], raw_df['volume'], label='Volume', color='orange')
axs[4].set_title('Volume Over Time')
axs[4].set_xlabel('Datetime')
axs[4].set_ylabel('Volume (Hundred Million)')

for ax in axs:
    ax.legend()

plt.figure(figsize=(5,20))
plt.show()

"""**Pairplot between each parwise group of features**

From the pairplot, note that by definition, the diagonals consist of histograms of a feature, whereas the off-diagonals consist of scatter plots plotting the partial correlation between two distinct features.

We can see that high, low, open and close are highly correlated with each other, but this is due to the fact that on average, the prices of AAPL stock during 30 minute intervals do not vary much, so there is a strong correlative relationship between these features. Therefore, in later consideration when training our algorithms, we will just select one of these features.

Additionally, because our high low open and close are also correlated, this indicates that AAPL stock volatility is relatively low in the short term.
"""

sns.pairplot(df, height=2)  # Decrease the height value to reduce the size of the plot

plt.suptitle("Pair Plot of DataFrame", y=1.02)  # Adjust the title location if needed
plt.show()

!pip install --upgrade mplfinance
import matplotlib.dates as mdates
from mplfinance.original_flavor import candlestick_ohlc

!pip install pandasql

"""Converting 30 minute plotted time series graph into a daily time series one."""

daily_df = raw_df.copy()
daily_df['date'] = daily_df['datetime'].dt.date

daily_df['datetime'] = pd.to_datetime(daily_df['datetime'])

# Group by date and aggregate open, high, low, close, and volume
daily_df = daily_df.groupby('date').agg({
    'open': 'first',
    'high': 'max',
    'low': 'min',
    'close': 'last',
    'volume': 'sum'
}).reset_index()

daily_df

sns.pairplot(daily_df)

plt.suptitle("Pair Plot of DataFrame (Daily Intervals)", y=1.02)
plt.show()

# Candlestick Chart

# Convert 'Date' column to matplotlib date format
matplotlib_date = mdates.date2num(daily_df['date'])

# Create an array of tuples in the required format
ohlc = np.vstack((matplotlib_date, daily_df['open'], daily_df['high'], daily_df['low'], daily_df['close'])).T

plt.figure(figsize=(15, 6))
ax = plt.subplot()
candlestick_ohlc(ax, ohlc, width=0.6, colorup='g', colordown='r')
ax.xaxis_date()
plt.title('Candlestick Chart')
plt.xlabel('Date')
plt.ylabel('Price')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

# Plotting EMA12, EMA26, MACD, and Signal Line in three separate graphs

# Calculate the Exponential Moving Averages for MACD
raw_df['EMA12'] = raw_df['close'].ewm(span=12, adjust=False).mean()
raw_df['EMA26'] = raw_df['close'].ewm(span=26, adjust=False).mean()

# Calculate the MACD and Signal line
raw_df['MACD'] = raw_df['EMA12'] - raw_df['EMA26']
raw_df['Signal'] = raw_df['MACD'].ewm(span=9, adjust=False).mean()

fig, axs = plt.subplots(3, 1, figsize=(14, 21))
fig.subplots_adjust(hspace=0.5)

# Plot EMA12 and EMA26
axs[0].plot(raw_df['datetime'], raw_df['EMA12'], label='EMA 12', color='blue')
axs[0].plot(raw_df['datetime'], raw_df['EMA26'], label='EMA 26', color='red')
axs[0].set_title('Exponential Moving Averages: EMA 12 and EMA 26')
axs[0].set_xlabel('Date')
axs[0].set_ylabel('Value')
axs[0].legend()

# Plot MACD
axs[1].plot(raw_df['datetime'], raw_df['MACD'], label='MACD', color='purple')
axs[1].set_title('Moving Average Convergence Divergence (MACD)')
axs[1].set_xlabel('Date')
axs[1].set_ylabel('Value')
axs[1].legend()

# Plot Signal Line
axs[2].plot(raw_df['datetime'], raw_df['Signal'], label='Signal Line', color='green')
axs[2].set_title('Signal Line')
axs[2].set_xlabel('Date')
axs[2].set_ylabel('Value')
axs[2].legend()

plt.show()

# Calculating the Relative Strength Index (RSI) for the 'close' price data

def calculate_rsi(data, window=14):
    # Compute price differences
    delta = data.diff()

    # Make two series: one for gains and one for losses
    gain = (delta.where(delta > 0, 0))
    loss = (-delta.where(delta < 0, 0))

    # Calculate the exponential moving average of gains and losses
    avg_gain = gain.ewm(com=window - 1, min_periods=window).mean()
    avg_loss = loss.ewm(com=window - 1, min_periods=window).mean()

    # Calculate the RSI
    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

# Calculate RSI for the dataset
raw_df['RSI'] = calculate_rsi(raw_df['close'])

# Plotting the RSI
plt.figure(figsize=(14, 7))
plt.plot(raw_df['datetime'], raw_df['RSI'], label='RSI', color='purple')
plt.title('Relative Strength Index (RSI)')
plt.xlabel('Date')
plt.ylabel('RSI')
plt.axhline(70, color='red', linestyle='--', linewidth=0.5)
plt.axhline(30, color='green', linestyle='--', linewidth=0.5)
plt.legend()
plt.show()

"""# **Data Visualizer **"""

from sklearn.linear_model import LinearRegression
from datetime import datetime
df['datetime'] = pd.to_datetime(df['datetime'])
df['year'] = df['datetime'].dt.year  # Extract year for easy filtering

# Filter Data by Year Range
def filter_data_by_year(df, start_year, end_year):
    return df[(df['year'] >= start_year) & (df['year'] <= end_year)]

# User Input
def get_user_input():
    start_year = int(input("Enter the start year: "))
    end_year = int(input("Enter the end year: "))
    feature = input("Enter the feature to plot (open, high, low, close, volume): ")
    return start_year, end_year, feature

# Plot Data and Fit Linear Regression
def plot_data_with_regression(df, feature):
    plt.figure(figsize=(10, 5))
    plt.scatter(df['datetime'], df[feature], color='blue', label='Data Points')

    # Prepare data for linear regression
    X = np.array(df['datetime'].map(datetime.toordinal)).reshape(-1, 1)  # Convert dates to ordinal
    y = df[feature].values
    model = LinearRegression()
    model.fit(X, y)
    trend_line = model.predict(X)

    plt.plot(df['datetime'], trend_line, color='red', linewidth=2, label='Trend Line')
    plt.title(f'{feature.capitalize()} Trend from {df["year"].min()} to {df["year"].max()}')
    plt.xlabel('Date')
    plt.ylabel(feature.capitalize())
    plt.legend()
    plt.show()

def main():
    start_year, end_year, feature = get_user_input()
    filtered_df = filter_data_by_year(df, start_year, end_year)

    if not filtered_df.empty:
        plot_data_with_regression(filtered_df, feature)
    else:
        print("No data available for the selected year range.")

if __name__ == "__main__":
    main()

"""#Data Preparation"""

#normalize
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df['scaled_close'] = scaler.fit_transform(df[['close']])

#prepare the sequences
def create_sequences(data, seq_length):
    xs = []
    ys = []
    for i in range(len(data)-seq_length-1):
        x = data.iloc[i:(i+seq_length)].values
        y = data.iloc[i+seq_length]
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

seq_length = 60  # Number of past records to consider
X, y = create_sequences(df['scaled_close'], seq_length)

print(y)

train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

history = model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

"""# Baseline Model: Simple Moving Average Predictor#

We made a simple moving average predictor as a baseline model, so that we can compare loss functions later on with the more complex models.
"""

# Assuming X and y are already defined as numpy arrays
def predict_average(X):
    # Compute the average for each sample across all features
    return np.mean(X, axis=1)

# Making predictions
y_pred = predict_average(X_train)

# Computing Mean Squared Error
mse = np.mean((y_train - y_pred)**2)

print(f"Mean Squared Error on Training Data: {mse}")

# Evaluating on the test set
y_pred_test = predict_average(X_test)
mse_test = np.mean((y_test - y_pred_test)**2)

print(f"Mean Squared Error on Test Data: {mse_test}")

#Baseline Model:

# Adjusting the SMA window to 5 periods (of 30 minutes) for the purpose of the example
raw_df['SMA_5'] = raw_df['close'].rolling(window=5).mean()

# The forecast for the next period is the last value of the SMA
next_point_prediction = raw_df['SMA_5'].iloc[-1]

# Display the predicted next closing price
next_point_prediction

"""# LSTM Network

For training, we decided to not use the train_test_split, but to chronologically split our data, with past data (approx. 80%) being our training data, and more recent data (approx. 20%) being our test data.
"""

#build the model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

model = Sequential([
    LSTM(50, return_sequences=True, input_shape=(X.shape[1], 1)),
    Dropout(0.2),
    LSTM(50, return_sequences=False),
    Dropout(0.2),
    Dense(1)
])
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])

history = model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1, validation_data=(X_test, y_test))

loss, mse = model.evaluate(X_test, y_test)
print("Loss:", loss)
print("Mean Squared Error:", mse)

predictions = model.predict(X_test)
print(predictions)  # Inverse transform to get actual values

print(y_test)

plt.figure(figsize=(12, 9))
# This plot shows our mdoels performance on AAPL test data

plt.plot(y_test, label='True Value')
plt.plot(predictions, label='Predicted Value')
plt.title('Prediction vs True Value')
plt.xlabel('Time Step')
plt.ylabel('AAPL Stock Price')
plt.legend()
plt.show()

"""# Random Forest"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

n_estimators_list = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
errors = []

for n_estimators in n_estimators_list:
    rf = RandomForestRegressor(n_estimators=n_estimators, random_state=42)
    rf.fit(X_train, y_train)
    y_pred = rf.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    errors.append(mse)
    print(f"Number of Estimators: {n_estimators}, MSE: {mse}")

plt.figure(figsize=(10, 6))
plt.plot(n_estimators_list, errors, marker='o')
plt.xlabel('Number of Estimators')
plt.ylabel('Mean Squared Error')
plt.title('Effect of Number of Estimators on MSE')
plt.grid(True)
plt.show()

"""#PCA Features for Prediction"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Assuming 'daily_df' contains features like 'open', 'high', 'low', 'close', 'volume'
features = daily_df[['open', 'high', 'low', 'close', 'volume']]
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Apply PCA
pca = PCA(n_components=3)  # Choosing 3 components for illustration
principal_components = pca.fit_transform(features_scaled)
daily_df[['PC1', 'PC2', 'PC3']] = principal_components

def create_sequences(data, seq_length):
    xs = []
    ys = []
    for i in range(len(data) - seq_length - 1):
        x = data.iloc[i:(i + seq_length)].values  # Selecting PCA features
        y = data.iloc[i + seq_length]  # This could be your target variable
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

# Assuming 'close' is the target and is outside of the PCA features
pca_features = daily_df[['PC1', 'PC2', 'PC3']]
target = daily_df['close'].shift(-1)  # Predict next day's close price

# Prepare sequences
seq_length = 60  # Number of past records to consider
X, y = create_sequences(pca_features, seq_length)

from sklearn.model_selection import train_test_split

train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Flatten the sequence data
X_train_reshaped = X_train.reshape(X_train.shape[0], -1)
X_test_reshaped = X_test.reshape(X_test.shape[0], -1)

# Now the data should be two-dimensional
print(X_train_reshaped.shape)  # Should print (samples, features*sequence_length)

# Train the RandomForestRegressor with the reshaped data
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train_reshaped, y_train)  # Use the reshaped training data
y_pred = model.predict(X_test_reshaped)  # Predict using the reshaped test data

from sklearn.metrics import mean_squared_error

mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

"""# ARIMA"""

from statsmodels.tsa.arima.model import ARIMA

# Convert the 'date' column to datetime type and set it as the index
df['datetime'] = pd.to_datetime(df['datetime'])
df.set_index('datetime', inplace=True)

# Work with the 'close' prices
data = df['close']

from statsmodels.tsa.stattools import adfuller

# Function to perform the Dickey-Fuller test
def test_stationarity(timeseries):
    dftest = adfuller(timeseries, autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])
    for key, value in dftest[4].items():
        dfoutput['Critical Value (%s)' % key] = value
    print(dfoutput)

# Check stationarity
test_stationarity(data)

# Differencing the data if necessary
data_diff = data.diff().dropna()
test_stationarity(data_diff)

# Split the data into train and test sets (80% train, 20% test)
split_point = int(len(data_diff) * 0.8)
train_data, test_data = data_diff[:split_point], data_diff[split_point:]

# Fit the ARIMA model
model = ARIMA(train_data, order=(1,1,1))  # Example: 1st order autoregressive model with 1st order differencing
results = model.fit()

# Summary of the model
print(results.summary())

# Predict future values
predictions = results.forecast(steps=len(test_data))
print(predictions)

plt.figure(figsize=(12,6))
plt.plot(data_diff, label='Original')
plt.plot(predictions, label='Predicted', color='red')
plt.legend(loc='best')
plt.show()

"""#Project Summary

**Problem Statement:** We are in a regression setting, trying to predict the next price of a stock given the previous price data of the stock. We chose Apple Stock 30-minute dataset over its lifetime because Apple is an important stock which many investors have a keen interest in, and it has some variance but isn't (visually, and as we've found) unpredictably variant.

**Dataset:** The dataset is a 30-minute interval Apple stock price dataset, with each interval containing high, low, open, and close prices on the interval as well as volume of stock exchanged over that interval.
Based on the actual application of our model, namely to predict stock prices, we made the decision to convert our data to daily instead of thirty-minute intervals, as learning daily prices would give investors more long-term and applicable predictions which they can make more impactful decisions on.

**EDA:** Our EDA was quite expansive and showed many different approaches to visualizing the data. We created time-series plots for datapoint open close, high, low, and volumes, we created Pairplots both on the thirty-minute dataset, then on the converted daily dataset we plotted EMA12 and EMA26 (which are exponential moving averages of 12 and 26 datapoints), MACD of these two (Moving Average Convergence Divergence) which shows whether bullish/bearish trends are strengthening/weakening, and Signal Line (9-day exponential moving average of the MACD), and lastly the Relative Strength Index (RSI) (speed and change of price movement), which are all very powerful stock visualization methods used by experts.

**Approach:** We first made a baseline regression model which simply took the average of all datapoints. Then, for our actual models, we created an LSTM Network, a Random Forest regressor, a PCA-feature Random Forest regressor, and an ARIMA model. For LSTM, we tested various hyperparameters such as size of hidden layers and found our current setup to be optimal. Moreover, for Random Forest we plotted the various losses at different number of estimators from 10 to 100, and found 30 to be the most reliable, which is why we used 30 regressors in our PCA random forest. Moreover, PCA learns a smaller dimensional space to represent the data, and thus we realized we could assists the regressor model by selecting only the most import featured through this preprocessing with PCA unsupervised learning method.

**Results:**
We used multiple models to find various methods of prediction, and based on the most reliable performance metric-- model loss -- our LSTM network performed the best and could predict price with quite high accuracy on test data (in the _e^-5 range). However, our Random Forest model is more interperetable as a model class, which can be more beneficial to some users.

**Conclusion and Discussion:** PCA was quite useful for Random Forest, which is a naturally interperetable model.

For LSTM, we found a nice sweet spot in terms of hidden layer size based on training time, overfitting, and underfitting. In terms of accurate results, users would likely prefer to use the LSTM over any other models. The model does take a nontrivial amount of time and GPU compute to train, especially compared to the others, however.

In the future we could consider adding an RNN to see how it compares to the LSTM, as RNNs are known to function well on time-series data (as are LSTMs)
"""